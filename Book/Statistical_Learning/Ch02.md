# 感知机模型

**Definition**

假设输入空间是$\mathcal{X} \subseteq \mathbf{R}^n$, 输出空间是$y= \{+1, -1\}$, 输入空间到输出空间的如下函数为：
$$
\Large 
f(x) = \text{sign}(w\cdot x + b), \text{sign} = \begin{cases} +1, x\geq 0 \\ -1,x<0   \end{cases}
$$
感知机是线性分类模型，它的假设空间是定义在特征空间中的所有线性分类模型。

**线性可分性**

给定一个数据集
$$
\Large
T = \{(x_1,y_1), (x_2,y_2),...,(x_n,y_n)\}
$$
其中$x_i\in\mathcal{X} = \mathbf{R}^n, y_i = \{+1, -1\}, i = 1,2,...,N$, 如果存在一个超平面$S$
$$
\Large w\cdot x +b = 0
$$
使得下式满足
$$
\Large
(w\cdot x_i + b) *y_i > 0
$$
则称数据集为线性可分，否则数据集线性不可分。

# 训练策略

损失函数为所有误分类点到超平面$S$的总距离，点$x_0$到超平面$S$的距离为
$$
\Large
\frac{1}{\left\lVert w\right\rVert}\lvert w\cdot x_0 + b\rvert
$$
因为对于误分类的数据下式成立
$$
\Large
(w\cdot x_i + b) *y_i < 0
$$
因此所求的距离都为负数，因此误分类点到超平面的距离为
$$
\Large
-\frac{1}{\left\lVert w\right\rVert}y_0*\lvert w\cdot x_0 + b\rvert
$$
假设超平面$S$的误分类点集合为$M$, 则误差为
$$
\Large
-\frac{1}{\left\lVert w\right\rVert}\sum\limits_{x_i\in M}y_i*\lvert w\cdot x_i + b\rvert
$$
去掉左侧的标量则为感知机的损失函数
$$
\Large 
L(w, b) = -\sum\limits_{x_i \in M} y_i(w\cdot x_i + b)
$$

# 学习算法

感知机学习算法的原始形式为求解以下损失函数极小化问题。
$$
\Large
\min\limits_{w,b} L(w, b) = -\sum\limits_{x_i \in M} y_i(w\cdot x_i + b)
$$
具体使用随机梯度下降法 (stochastic gradient descent) 来求解。

任取$(w_0, b_0)$作为随机的超平面，计算梯度
$$
\Large
\begin{align}
\nabla_w L(w,b) &= - \sum\limits_{x_i \in M} y_i x_i\\
\nabla_b L(w,b) &= - \sum\limits_{x_i \in M} y_i
\end{align}
$$
更新$(w,b)$
$$
\Large
\begin{align}
w &\leftarrow w + \eta y_ix_i\\
b &\leftarrow b + \eta y_i
\end{align}
$$
算法形式如下

**输入**：数据集 $T = \{(x_1,y_1), (x_2,y_2), ..., (x_N, y_N)\}, x_i \in \mathcal{X} = \mathbf{R}^n, y_i \in \mathcal{Y} = \{+1,-1\}, i = 1,2,...,N, \eta \in(0, 1]$

**输出**：$(w,b)$， 感知机模型 $f(x) = \text{sign}(w\cdot x + b)$

1. 选取初值 $(w_0, b_0)$
2. 从训练集中选择数据 $(x_i, y_i)$
3. 如果 $y_i(w\cdot x_i + b) \leq 0$，则根据上式更新 $(w, b)$
4. 转至步骤2， 直至没有误分类点



































