# 感知机模型

**Definition**

假设输入空间是$\mathcal{X} \subseteq \mathbf{R}^n$, 输出空间是$y= \{+1, -1\}$, 输入空间到输出空间的如下函数为：
$$
\Large 
f(x) = \text{sign}(w\cdot x + b), \text{sign} = \begin{cases} +1, x\geq 0 \\ -1,x<0   \end{cases}
$$
感知机是线性分类模型，它的假设空间是定义在特征空间中的所有线性分类模型。

**线性可分性**

给定一个数据集
$$
\Large
T = \{(x_1,y_1), (x_2,y_2),...,(x_n,y_n)\}
$$
其中$x_i\in\mathcal{X} = \mathbf{R}^n, y_i = \{+1, -1\}, i = 1,2,...,N$, 如果存在一个超平面$S$
$$
\Large w\cdot x +b = 0
$$
使得下式满足
$$
\Large
(w\cdot x_i + b) *y_i > 0
$$
则称数据集为线性可分，否则数据集线性不可分。

# 训练策略

损失函数为所有误分类点到超平面$S$的总距离，点$x_0$到超平面$S$的距离为
$$
\Large
\frac{1}{\left\lVert w\right\rVert}\lvert w\cdot x_0 + b\rvert
$$
因为对于误分类的数据下式成立
$$
\Large
(w\cdot x_i + b) *y_i < 0
$$
因此所求的距离都为负数，因此误分类点到超平面的距离为
$$
\Large
-\frac{1}{\left\lVert w\right\rVert}y_0*\lvert w\cdot x_0 + b\rvert
$$
假设超平面$S$的误分类点集合为$M$, 则误差为
$$
\Large
-\frac{1}{\left\lVert w\right\rVert}\sum\limits_{x_i\in M}y_i*\lvert w\cdot x_i + b\rvert
$$
去掉左侧的标量则为感知机的损失函数
$$
\Large 
L(w, b) = -\sum\limits_{x_i \in M} y_i(w\cdot x_i + b)
$$

# 学习算法

感知机学习算法的原始形式为求解以下损失函数极小化问题。
$$
\Large
\min\limits_{w,b} L(w, b) = -\sum\limits_{x_i \in M} y_i(w\cdot x_i + b)
$$
具体使用随机梯度下降法 (stochastic gradient descent) 来求解。

任取$(w_0, b_0)$作为随机的超平面，计算梯度
$$
\Large
\begin{align}
\nabla_w L(w,b) &= - \sum\limits_{x_i \in M} y_i x_i\\
\nabla_b L(w,b) &= - \sum\limits_{x_i \in M} y_i
\end{align}
$$
更新$(w,b)$
$$
\Large
\begin{align}
w &\leftarrow w + \eta y_ix_i\\
b &\leftarrow b + \eta y_i
\end{align}
$$
算法形式如下

**输入**：数据集 $T = \{(x_1,y_1), (x_2,y_2), ..., (x_N, y_N)\}, x_i \in \mathcal{X} = \mathbf{R}^n, y_i \in \mathcal{Y} = \{+1,-1\}, i = 1,2,...,N, \eta \in(0, 1]$

**输出**：$(w,b)$， 感知机模型 $f(x) = \text{sign}(w\cdot x + b)$

1. 选取初值 $(w_0, b_0)$
2. 从训练集中选择数据 $(x_i, y_i)$
3. 如果 $y_i(w\cdot x_i + b) \leq 0$，则根据上式更新 $(w, b)$
4. 转至步骤2， 直至没有误分类点





# 收敛性

通过有限次迭代可以得到一个将训练集完全正确划分的分离超平面及感知机模型。

**Novikoff 定理**

设训练数据集 $T = \{(x_1,y_1), (x_2, y_2),...,(x_N,y_N)\}$ 是线性可分的，

存在满足条件 $\left\lVert \hat{W}_{\text{opt}} \right\rVert = 1$ 的超平面 $\hat{W}_{\text{opt}}\cdot \hat{x} = w_{\text{opt}}\cdot x + b_{\text{opt}} = 0$ 将训练数据集完全正确分开，且存在 $\gamma > 0$， 对所有的$i =1,2,...,N$
$$
\Large
y_i(\hat{w}_{\text{opt}}\cdot \hat{x}) = y_i (w_{\text{opt}}\cdot x + b_{\text{opt}}) \geq \gamma
$$
令 $R = \max\limits_{1\leq i \leq N} \left\lVert \hat{x}_i \right\rVert$， 则感知机算法在训练数据集上的误分类次数 $k$ 满足不等式
$$
\Large
k \leq \left(\frac{R}{\gamma}\right)^2
$$

**证明1**

存在一个超平面  $\hat{w}_{opt}$  将数据完全正确的分开, 且 $\left\lVert \hat{w}_{opt}\right\rVert = 1$, 对任意 $i=1, 2, 3,...,N$ 都有
$$
\Large
y_i (\hat{w}_{opt} \cdot \hat{x}_i) = y_i (w_{opt}\cdot x_i + b_{opt}) > 0
$$
因此对于最近距离的数据点，有如下公式
$$
\Large
\gamma = \min\limits_{i}\{y_i (w_{opt}\cdot x_i + b_{opt})\}, \left\lVert \hat{w}_{opt}\right\rVert = 1
$$
 对任意 $i=1, 2, 3,...,N$ 都有
$$
\Large
y_i (\hat{w}_{opt} \cdot \hat{x}_i) = y_i (w_{opt}\cdot x_i + b_{opt}) \geq \gamma
$$

**证明2**

感知机从 $\hat{w}_0 = 0$ 开始，令 $\hat{w}_{k-1}$是第$k$个误分类实例之前的扩充权重向量。
$$
\Large
\hat{w}_{k-1} = (w_{k-1}^T, b_{k-1})^T
$$
第$k$个误分类实例的条件是
$$
\Large
y_i(\hat{w}_{k-1} \cdot \hat{x}) = y_i (w_{k-1} \cdot x_i + b_{k-1}) \leq 0
$$
若$(x_i, y_i)$是被误分类的数据，则$(w, b)$ 更新
$$
\Large
\begin{align}
w_k &= w_{k-1} + \eta y_i x_i\\
b_k &= b_{k-1} + \eta y_i\\
\hat{w}_k &= \hat{w}_{k-1} +\eta y_i \hat{x}_i
\end{align}
$$
由下式可得
$$
\Large
\begin{align}
\hat{w}_{k} \cdot \hat{w}_{opt} = \hat{w}_{k-1} \cdot \hat{w}_{opt} + \eta y_i \hat{w}_{opt} \cdot \hat{x}_i &\geq \hat{w}_{k-1} \cdot \hat{w}_{opt} + \eta\gamma\\
&\geq \hat{w}_{0} \cdot \hat{w}_{opt} + k\eta \gamma \\
&= k\eta\gamma

\end{align}
$$

$$
\Large 
\begin{align}
\lVert \hat{w}_k \rVert^2 &= \lVert \hat{w}_{k-1} \rVert^2 + 2\eta y_i \hat{w}_{k-1} \cdot \hat{x}_{i} + \eta^2 \lVert \hat{x}_i \rVert^2\\
&\leq \lVert \hat{w}_{k-1} \rVert^2 +  \eta^2 \lVert \hat{x}_i \rVert^2\\
&\leq \lVert \hat{w}_{k-1} \rVert^2 +  \eta^2 R^2\\
&\leq  \lVert \hat{w}_{0} \rVert^2 + k\eta^2R^2 = k\eta^2R^2 
\end{align}
$$


结合上式，可得
$$
\Large
k\eta\gamma \leq \hat{w}_k \cdot \hat{w}_{opt} \leq \lVert \hat{w}_k \rVert \lVert \hat{w}_{opt} \rVert \leq \sqrt{k}\eta R\\
\Large k^2\gamma^2 \leq kR^2\\
\Large k\leq \left(\frac{R}{\gamma}\right)^2
$$

# 感知机对偶形式

每次的更新都与数据点相关，因此最终的$(w,b)$可以写为如下形式
$$
\Large
\begin{align}
w &= \sum\limits_{i=1}^{N}\alpha_i y_i x_i\\
b &= \sum\limits_{i=1}^{N} \alpha_i y_i
\end{align}
$$
其中 $\alpha_i$ 表示第 $i$ 个实例点由于误分类二进行更新的次数。算法形式如下

**输入**：数据集 $T = \{(x_1,y_1), (x_2,y_2), ..., (x_N, y_N)\}, x_i \in \mathcal{X} = \mathbf{R}^n, y_i \in \mathcal{Y} = \{+1,-1\}, i = 1,2,...,N, \eta \in(0, 1]$

**输出**：$(\alpha,b)$， 感知机模型 $\Large f(x) = \text{sign}\left(\sum\limits_{j=1}^{N}\alpha_j y_j x_j \cdot x + b\right)$, $\alpha = (\alpha_1, \alpha_2, ..., \alpha_N)^T$



1. $\Large (\alpha, b) = (0, 0)$

2. 从训练集中选数据 $\Large (x_i, y_i)$

3. 如果 $\Large y_i \left(\sum\limits_{j=1}^{N}\alpha_j y_j x_j \cdot x_i + b \right)\leq 0$
   $$
   \Large
   \begin{align}
   \alpha_i &\leftarrow \alpha_i + \eta\\
   b &\leftarrow b + \eta y_i
   \end{align}
   $$

4. 转到步骤2，直至没有错误出现



# 习题

## 2.1

XOR:

| $x_1$ | $x_2$ | $y$  |
| ----- | ----- | ---- |
| 0     | 0     | 0    |
| 0     | 1     | 1    |
| 1     | 0     | 1    |
| 1     | 1     | 0    |

It is not linear separable.

## 2.3













