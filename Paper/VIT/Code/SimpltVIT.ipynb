{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14a5a901-95ab-4326-a12d-60edaf3c052e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import optim\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.set_default_tensor_type(torch.DoubleTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1dc788f9-89cd-4c74-9665-4ae6dd2281cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_CIFAR_batch(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        datadict = pickle.load(f,encoding='bytes')\n",
    "    return datadict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9712daa8-d9ed-4ca8-8dac-98a666199a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_CIFAR_batch(\"/home/yang/Dataset/cifar-100-python/train\")\n",
    "X = dataset[b'data']\n",
    "Y = dataset[b'fine_labels'];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "765a641f-0ad2-4c43-89ff-856fe04c06db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 3, 32, 32)\n",
      "(50000,)\n"
     ]
    }
   ],
   "source": [
    "X = np.array(X)\n",
    "X = X.reshape((50000, 3,32,32)).astype(int)\n",
    "Y = np.array(Y)\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba9a77e3-9b27-449e-8b5b-4d5b55d31218",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3662494511.py, line 16)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [12]\u001b[0;36m\u001b[0m\n\u001b[0;31m    x = self.proj(x).\u001b[0m\n\u001b[0m                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class VIT(nn.Module):\n",
    "    def __init__(self, img_size = 32, in_c = 3, num_classes = 100, embed_dim = 768, patch_size=4):\n",
    "        super(VIT, self).__init__()\n",
    "        self.img_size = img_size\n",
    "        self.in_c = in_c\n",
    "        self.num_classes = num_classes\n",
    "        self.embed_dim = embed_dim\n",
    "        self.patch_size = patch_size\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.proj = nn.Conv2d(in_c, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        \n",
    "    def patch_embed(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        x = self.proj(x).\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf2e70f-7d5a-4fa1-abe8-bb2fd5a8f032",
   "metadata": {},
   "source": [
    "# Part 1\n",
    "Generate the image patch, for a $32\\times32$ image, we set patch size is $4\\times4$, and output dimension is 768.\n",
    "\n",
    "The patch embedding process can be simpliy as a single layer CNN\n",
    "\n",
    "The 50000 image is a big burden for computer, so we only choose top 100 image for computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33e9de2a-4382-4b8e-9088-577624839822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 3, 32, 32])\n",
      "torch.DoubleTensor\n"
     ]
    }
   ],
   "source": [
    "X1 = X[:100, :, :, :]\n",
    "tensor_x = torch.from_numpy(X1).type(torch.double)\n",
    "print(tensor_x.shape)\n",
    "print(tensor_x.type())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949926b8-ec8a-4bc0-a3c5-5d3b97e728c8",
   "metadata": {},
   "source": [
    "apply CNN to generate the image embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79bef295-72f9-48f1-a27b-4be274cf5591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 768, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "B, C, H, W = tensor_x.shape\n",
    "A1 = nn.Conv2d(in_channels=3, out_channels=768, kernel_size=4, stride=4)\n",
    "tensor_x1 = A1(tensor_x)\n",
    "print(tensor_x1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795aff9d-6968-430f-beda-b50d3ae1dfbe",
   "metadata": {},
   "source": [
    "Flatten the H and W channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "010768c5-e4a6-405d-ab05-19a1e1b9163a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 768, 64])\n"
     ]
    }
   ],
   "source": [
    "tensor_x2 = tensor_x1.flatten(2)\n",
    "print(tensor_x2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ab7091-c033-447a-aac0-f26ac74f30ea",
   "metadata": {},
   "source": [
    "exchange the Channel and patch dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19ce85b0-6ec4-446a-b561-f4c4b33f4684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 64, 768])\n"
     ]
    }
   ],
   "source": [
    "tensor_x3 = tensor_x2.transpose(1,2)\n",
    "print(tensor_x3.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52909be4-5cf4-46a8-863b-0fffe2799b27",
   "metadata": {},
   "source": [
    "# Part 2: Add class token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4647db8c-edc4-4fca-88aa-492924982c54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 1, 768])\n"
     ]
    }
   ],
   "source": [
    "cls_token = nn.Parameter(torch.zeros(1, 1, 768))\n",
    "cls_token = cls_token.expand(tensor_x3.shape[0], -1, -1)\n",
    "print(cls_token.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c1bb53f0-980e-4930-808a-3f2fd1f2d819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 65, 768])\n"
     ]
    }
   ],
   "source": [
    "tensor_x4 = torch.cat((cls_token, tensor_x3), dim=1)\n",
    "print(tensor_x4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0baee28-d086-46ef-9fbf-ab99c8f129f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
