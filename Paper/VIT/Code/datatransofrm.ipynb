{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44fa6b1d-5117-4c9c-95a8-73e8fd1df419",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11a65300-39a5-40dd-953c-f8e2dda87836",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680874d7-6047-478d-a8a1-b1b81e8e08a8",
   "metadata": {},
   "source": [
    "# Patch embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00655df9-cda9-43ba-8bd2-87444f05f36f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "# B, C, H, W\n",
    "img = torch.rand((100, 3, 224, 224))\n",
    "print(img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed60b77e-5a46-49d8-8f00-40832e7cb52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = (224, 224)\n",
    "patch_size = (16, 16)\n",
    "grid_size = (img_size[0]//patch_size[0], img_size[1]//patch_size[1])\n",
    "num_patches = grid_size[0] * grid_size[1]\n",
    "proj = nn.Conv2d(3, 768, kernel_size=patch_size, stride=patch_size)\n",
    "norm = nn.Identity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a481661-13fe-45d5-b95e-7a334eb01fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch, embed_dim, H_Patch, W_patch]\n",
      "torch.Size([100, 768, 14, 14])\n",
      "[Batch, embed_dim, num_patch]\n",
      "torch.Size([100, 768, 196])\n",
      "[Batch, Patch, embed_dim]\n",
      "torch.Size([100, 196, 768])\n"
     ]
    }
   ],
   "source": [
    "B, C, H, W = img.shape\n",
    "x = proj(img)\n",
    "print(\"[Batch, embed_dim, H_Patch, W_patch]\")\n",
    "print(x.shape)\n",
    "x = x.flatten(2)\n",
    "print(\"[Batch, embed_dim, num_patch]\")\n",
    "print(x.shape)\n",
    "x = x.transpose(1,2)\n",
    "print(\"[Batch, Patch, embed_dim]\")\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ded6c6-48c0-4ef4-b177-a80b604de10d",
   "metadata": {},
   "source": [
    "# Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e9c44692-55f1-42cb-9eb3-b0addbde5fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_heads = 16\n",
    "dim = 768\n",
    "head_dim = dim//num_heads\n",
    "qkv = nn.Linear(dim, dim*3, bias = False)\n",
    "scale = head_dim ** -0.5\n",
    "attn_drop = 0.2\n",
    "proj1 = nn.Linear(dim, dim)\n",
    "proj_drop = nn.Dropout(0.2)\n",
    "proj2 = nn.Linear(dim, dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "05ee6c0b-1123-450d-8181-5abc965cfe5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 1, 768])\n"
     ]
    }
   ],
   "source": [
    "cls_token = nn.Parameter(torch.zeros(1, 1, 768))\n",
    "cls_token = cls_token.expand(x.shape[0], -1, -1)\n",
    "print(cls_token.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "39378d80-01c0-447c-af07-f73d30b4fc72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 197, 768])\n"
     ]
    }
   ],
   "source": [
    "x = torch.cat((cls_token, x), dim=1)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "010c1a01-2762-42f7-9792-1aab6b85143c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[B, num_patches+1, embed_dim]\n",
      "torch.Size([100, 197, 768])\n",
      "[B, num_patchs+1, qkv_dim]\n",
      "torch.Size([100, 197, 2304])\n",
      "[B, num_patch+1, qkv, num_heads, heads_dim]\n",
      "torch.Size([100, 197, 3, 16, 48])\n",
      "[3, batch_size, num_heads, num_patches + 1, embed_dim_per_head]\n",
      "torch.Size([3, 100, 16, 197, 48])\n"
     ]
    }
   ],
   "source": [
    "B, N, C = x.shape\n",
    "print(\"[B, num_patches+1, embed_dim]\")\n",
    "print(x.shape)\n",
    "x = qkv(x)\n",
    "print(\"[B, num_patchs+1, qkv_dim]\")\n",
    "print(x.shape)\n",
    "x = x.reshape(B, N, 3, num_heads, C//num_heads)\n",
    "print(\"[B, num_patch+1, qkv, num_heads, heads_dim]\")\n",
    "print(x.shape)\n",
    "x = x.permute(2, 0, 3, 1, 4)\n",
    "print(\"[qkv, batch_size, num_heads, num_patches + 1, embed_dim_per_head]\")\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7d9dc671-7496-448a-93dc-3eb4ff20837e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q:  torch.Size([100, 16, 197, 48])\n",
      "k:  torch.Size([100, 16, 197, 48])\n",
      "v:  torch.Size([100, 16, 197, 48])\n"
     ]
    }
   ],
   "source": [
    "q, k, v = x[0], x[1], x[2]\n",
    "print(\"q: \", q.shape)\n",
    "print(\"k: \", k.shape)\n",
    "print(\"v: \", v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bfc15cc4-afd9-4c93-b235-25208a22c42a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 16, 197, 197])\n"
     ]
    }
   ],
   "source": [
    "attn = (q @ k.transpose(-2, -1))\n",
    "attn = attn.softmax(dim=-1)\n",
    "print(attn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a21b2dbb-fb10-4be5-b4ba-f01f07a4c734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 197, 768])\n"
     ]
    }
   ],
   "source": [
    "x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "x = proj2(x)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d5fa3b-849a-4858-8b72-46bec29054a3",
   "metadata": {},
   "source": [
    "# MyCode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f434685-a53e-43bd-8b12-59b869725d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from vit_model import Attention\n",
    "from vit_model import PathEmbed\n",
    "from vit_model import Attention\n",
    "from vit_model import MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27d8e889-f599-496c-8dfd-57c9c72cdc3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 196, 768])\n"
     ]
    }
   ],
   "source": [
    "img = torch.rand((100, 3, 224, 224))\n",
    "embeddingLayer = PathEmbed()\n",
    "x = embeddingLayer(img)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6fa57ed-56e8-4e41-9c8d-caf2d1c0e244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 196, 768])\n"
     ]
    }
   ],
   "source": [
    "multiHeadAttention = Attention(dim=768, num_heads=16)\n",
    "x = multiHeadAttention(x)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bde26db0-9257-4c02-8fc7-61d4565635e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 196, 768])\n"
     ]
    }
   ],
   "source": [
    "mlp = MLP(in_features=768)\n",
    "x = mlp(x)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6588a7ec-cd70-45dd-83e7-a572114d98b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vit_model import vit_base_patch16_224_in21k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e898f139-ac22-4b55-9d01-94b0972d6135",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = vit_base_patch16_224_in21k(has_logits=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb10a22d-2dfe-4551-9112-7aac3efb65f8",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2821267061.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [3]\u001b[0;36m\u001b[0m\n\u001b[0;31m    X = model.for(img)\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "img = torch.rand((100, 3, 224, 224))\n",
    "X = model.forward(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9dff273-3bd6-4e4b-bb50-976085e12d65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
