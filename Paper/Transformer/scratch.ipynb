{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0.dev20230603+cu121\n"
     ]
    }
   ],
   "source": [
    "# importing required libraries\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import math,copy,re\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torchtext\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.simplefilter(\"ignore\")\n",
    "print(torch.__version__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bastic components\n",
    "## Create word embeddings\n",
    "\n",
    "Assume embedding vector is $512$ dimension, vocab size is $100$. Then embedding matrix size wil be $512\\times100$.\n",
    "\n",
    "If batch size is $64$, and sequence length is $10$, the output of embedding will be $64\\times10\\times512$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size):\n",
    "        super(Embedding, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.embed(x)\n",
    "        return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Encoding\n",
    "\n",
    "$$\n",
    "\\Large\\begin{align}\n",
    "\\text{PE}_{(\\text{pos}, 2i)} &= \\sin(\\frac{\\text{pos}}{10000^{2i/ d_{\\text{model}}}}) \\\\\n",
    "\\text{PE}_{(\\text{pos}, 2i+1)} &= \\cos(\\frac{\\text{pos}}{10000^{2i/ d_{\\text{model}}}})\n",
    "\\end{align}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEmbedding(nn.Module):\n",
    "    def __init__(self, max_seq_len, embed_model_dim):\n",
    "        super(PositionEmbedding, self).__init__()\n",
    "        self.emebd_dim = embed_model_dim\n",
    "        pe = torch.zeros(max_seq_len, self.embed_size)\n",
    "        for pos in range(max_seq_len):\n",
    "            for i in range(0, self.emebd_dim, 2):\n",
    "                pe[pos, i] = math.sin(pos/(10000**((2*i)/self.emebd_dim)))\n",
    "                pe[pos, i+1] = math.cos(pos/(10000**((2*i)/self.emebd_dim)))\n",
    "\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
