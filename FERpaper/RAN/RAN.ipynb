{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4a8e0e3-d37d-420c-94fa-e9afb785ebe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67bc510c-ae86-4968-8c1b-b759b0acad37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = np.load(\"/home/viper/Downloads/X_basic.npy\", allow_pickle=True)\n",
    "Y = np.load(\"/home/viper/Downloads/Y_basic.npy\", allow_pickle=True)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "X_test, X_valid, Y_test, Y_valid = train_test_split(X_test, Y_test, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce1d906-254e-44fc-890b-4414c4ed7504",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3115a6-6fc8-4cdc-9468-f15499384d6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "8da68251-10d1-42d7-86a3-f6b84590a760",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAN(nn.Module):\n",
    "    def __init__(self, net):\n",
    "        super(RAN, self).__init__()\n",
    "        self.feature = net\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.alpha = nn.Sequential(nn.Linear(512, 1),\n",
    "                                   nn.Sigmoid())\n",
    "        self.beta = nn.Sequential(nn.Linear(1024, 1),\n",
    "                                  nn.Sigmoid())\n",
    "        self.fc = nn.Linear(1024, 7)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 3, 224, 224)\n",
    "        # Extract feature module\n",
    "        ## X: [B, 6, 512]\n",
    "        res1 = self.feature(x).squeeze(3).squeeze(2).view(-1, 6, 512)\n",
    "        print(\"res1: \", res1.shape)\n",
    "        # Self attention module\n",
    "        ## mu\n",
    "        mu = self.alpha(res1.view(-1, 512)).view(-1, 6, 1)\n",
    "        mu = F.softmax(mu, dim=1)\n",
    "        print(\"mu: \", mu.shape)\n",
    "        mu_max = mu[:, 0:5, :].max(dim=1)[0]\n",
    "        mu_org = mu[:, 5, :]\n",
    "        ## Fm\n",
    "        Fm = res1.mul(mu).sum(1).div(mu.sum(1)).unsqueeze(dim=1)\n",
    "        print(\"Fm: \", Fm.shape)\n",
    "        \n",
    "        # Relation-attention module\n",
    "        ## concat Fi:Fm\n",
    "        res2 = torch.cat((res1, Fm.repeat(1, 6, 1)), dim=2).view(-1, 6, 1024)\n",
    "        print(\"res2: \", res2.shape)\n",
    "        ## vi\n",
    "        vi = self.beta(res2).view(-1, 6)\n",
    "        vi = F.softmax(vi, dim=1)\n",
    "        print(\"vi: \", vi.shape)\n",
    "        \n",
    "        ## PRAN\n",
    "        PRAN = res2.mul((mu.squeeze() * vi).unsqueeze(2)).sum(dim=1).div((mu.squeeze() * vi).sum(dim=1).unsqueeze(dim=1))\n",
    "        res3 = self.fc(PRAN)\n",
    "        \n",
    "        # res\n",
    "        return res3\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "9595bfa2-d4cd-49f2-a0f2-9102a5d37089",
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone = nn.Sequential(*list(models.resnet18(pretrained=True).children())[:-1])\n",
    "model = RAN(backbone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "61581f72-eed1-4d0e-b6dd-69be5d133636",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaffeCrop(object):\n",
    "\n",
    "    def __init__(self, phase):\n",
    "        assert(phase=='train' or phase=='test')\n",
    "        self.phase = phase\n",
    "\n",
    "    def __call__(self, img):\n",
    "        # pre determined parameters\n",
    "        final_size = 224\n",
    "        final_width = final_height = final_size\n",
    "        crop_size = 110\n",
    "        crop_height = crop_width = crop_size\n",
    "        crop_center_y_offset = 15\n",
    "        crop_center_x_offset = 0\n",
    "        if self.phase == 'train':\n",
    "            scale_aug = 0.02\n",
    "            trans_aug = 0.01\n",
    "        else:\n",
    "            scale_aug = 0.0\n",
    "            trans_aug = 0.0\n",
    "        \n",
    "        # computed parameters\n",
    "        randint = random.randint\n",
    "        scale_height_diff = (randint(0,1000)/500-1)*scale_aug\n",
    "        crop_height_aug = crop_height*(1+scale_height_diff)\n",
    "        scale_width_diff = (randint(0,1000)/500-1)*scale_aug\n",
    "        crop_width_aug = crop_width*(1+scale_width_diff)\n",
    "\n",
    "\n",
    "        trans_diff_x = (randint(0,1000)/500-1)*trans_aug\n",
    "        trans_diff_y = (randint(0,1000)/500-1)*trans_aug\n",
    "\n",
    "\n",
    "        center = ((img.width/2 + crop_center_x_offset)*(1+trans_diff_x),\n",
    "                 (img.height/2 + crop_center_y_offset)*(1+trans_diff_y))\n",
    "\n",
    "        \n",
    "        if center[0] < crop_width_aug/2:\n",
    "            crop_width_aug = center[0]*2-0.5\n",
    "        if center[1] < crop_height_aug/2:\n",
    "            crop_height_aug = center[1]*2-0.5\n",
    "        if (center[0]+crop_width_aug/2) >= img.width:\n",
    "            crop_width_aug = (img.width-center[0])*2-0.5\n",
    "        if (center[1]+crop_height_aug/2) >= img.height:\n",
    "            crop_height_aug = (img.height-center[1])*2-0.5\n",
    "\n",
    "        crop_box = (center[0]-crop_width_aug/2, center[1]-crop_height_aug/2,\n",
    "                    center[0]+crop_width_aug/2, center[1]+crop_width_aug/2)\n",
    "\n",
    "        mid_img = img.crop(crop_box)\n",
    "        res_img = img.resize( (final_width, final_height) )\n",
    "        return res_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "0ca15acb-836a-4a8c-8fbc-f0e907cb2158",
   "metadata": {},
   "outputs": [],
   "source": [
    "img =  Image.open(\"/home/viper/Downloads/face_f.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed9ae93-064f-4a92-a72a-a6c81b8a1428",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2c09a9-1b92-406f-b1cd-59bd027a9a63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e46efbd-a1d2-4857-92d9-8bc55c8c0033",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
